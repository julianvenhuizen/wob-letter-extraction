{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5887fd",
   "metadata": {},
   "source": [
    "# Feature extraction on _Wet openbaarheid bestuur_ (Wob) decision letters\n",
    "\n",
    "The aim of this study is to research and apply knowledge extraction methods for multiple features of the Wob decision letter. While the decision letters usually follow the same structure, there are some differences to be found. These differences are mainly due to administrative bodies varying in the way they draft these letters.\n",
    "\n",
    "There are six features that are commonly found in the letters, namely: the document list, the relevant articles of law, the request of the applicant, the final decision by the administrative body, the date of the decision and the date of the request. These features each warrant their own way of extraction and method of evaluation. For example, the document list is often a table and can be extracted by algorithmic table detection. The articles of law, on the other hand, can be obtained by using rule-based text extraction. The request and decision parts of the letter can be extracted using text segmentation. Finally, the dates relating to the request, decision and proceedings can be obtained using named-entity recognition. Figure 1 shows an example of a standard Wob decision letter, with the features highlighted.\n",
    "\n",
    "Juli√°n Venhuizen (julian.venhuizen@student.uva.nl)\n",
    "\n",
    "<figure style=\"margin-top: 20px;\">\n",
    "    <img src=\"img/Annotated Wob decision letter.png\" alt=\"Wob decision letter (annotated)\"/>\n",
    "    <figcaption style=\"margin: 20px 60px; text-align: center;\">\n",
    "        <strong><em>Figure 1. An annotated Wob decision letter showing the features we want to extract. Red: date of decision. Orange: date of request. Yellow: request. Green: decision. Blue: articles of law. Purple: inventory table.</em></strong>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a33fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694144be",
   "metadata": {},
   "source": [
    "### Setting up variables\n",
    "First, we load some variables that will be used throughout the extraction process. These are the Dutch stopwords from NLTK and the Dutch spaCy model.\n",
    "\n",
    "For faster but less accurate results, the spaCy model could be changed to ```nl_core_news_sm```. More info can be found on https://spacy.io/models/nl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eed323",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('dutch')\n",
    "nlp = spacy.load('nl_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b999454",
   "metadata": {},
   "source": [
    "Set the ```SCRIPTS``` variable to the location of the scripts, the ```DATA``` variable to the path of the test dataset and the ```GROUND_TRUTH``` variable to the csv file containing the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPTS = './scripts'\n",
    "DATA = './data/WooIR_clean'\n",
    "GROUND_TRUTH = './data/GT/WooIR_clean.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda47ba9",
   "metadata": {},
   "source": [
    "### Creating a dataframe of the TEST data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e654b40",
   "metadata": {},
   "source": [
    "We also set the ```THRESHOLD``` for the percentage of text the document should contain. The default value is 3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d33b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677b3f0",
   "metadata": {},
   "source": [
    "We first generate an index of the data, then we clean and filter the data using some predefined rules and finally we preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.collection import generate_index, filter_and_clean_index\n",
    "from scripts.preparation import preprocess\n",
    "\n",
    "df = generate_index(DATA)\n",
    "df = filter_and_clean_index(df)\n",
    "df = preprocess(df, THRESHOLD)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7d7ef",
   "metadata": {},
   "source": [
    "### Extracting the features\n",
    "Then, we extract the features. We extract the decision date, the request date, the decision, the request, the articles of law and the table pages. The table pages are extracted twice: once using extra logic, once without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.extraction import extract_features\n",
    "\n",
    "df = extract_features(df, nlp)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766d88f",
   "metadata": {},
   "source": [
    "### Evaluating and scoring the results\n",
    "Finally we evaluate the dataframe. We do this by first loading the ground truth into a dataframe. The ground truth for WooIR was manually extracted from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv(GROUND_TRUTH)\n",
    "# display(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efc699",
   "metadata": {},
   "source": [
    "When calculating the score, we can pass a name for the output folder and a name for the dataset. The output folder will be used to store the generated plots and the dataset name will be used in score report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluation import evaluate, score\n",
    "\n",
    "ev = evaluate(df, gt, nlp)\n",
    "score(ev, output_folder='img', dataset='WooIR')\n",
    "# display(ev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
